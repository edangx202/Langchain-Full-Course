{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "In some applications like chatbots, it is important to remember previous interactions to keep the whole context of a conversation.\n",
    "Memory does provide you an easy way to handle this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi!', additional_kwargs={}, example=False),\n",
       " AIMessage(content='hello my friend!', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"hi!\")\n",
    "history.add_ai_message(\"hello my friend!\")\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: hi!\\nAI: hello my friend!'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"hi!\")\n",
    "memory.chat_memory.add_ai_message(\"hello my friend!\")\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hi there! How can I help you?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, verbose=True, memory=ConversationBufferMemory()\n",
    ")\n",
    "conversation.predict(input=\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi\n",
      "AI:  Hi there! How can I help you?\n",
      "Human: I need to know the capital of france\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The capital of France is Paris. It is located in the northern part of the country and is the most populous city in France. It is also the cultural and economic center of the country.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I need to know the capital of france\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory\n",
    "When inputs get long, we might not want to send the whole conversation, but rather a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2022.1.18\n",
      "  Downloading regex-2023.6.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.9/781.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tiktoken) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2023.6.3 tiktoken-0.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "review = \"I ordered Pizza Salami for 9.99$ and it was awesome! \\\n",
    "The pizza was delivered on time and was still hot when I received it. \\\n",
    "The crust was thin and crispy, and the toppings were fresh and flavorful. \\\n",
    "The Salami was well-cooked and complemented the cheese perfectly. \\\n",
    "The price was reasonable and I believe I got my money's worth. \\\n",
    "Overall, I am very satisfied with my order and I would recommend this pizza place to others.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hello, how can I help you today?\"},\n",
    "    {\"output\": \"Could you analyze a review for me?\"},\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Sure, I'd be happy to. Could you provide the review?\"},\n",
    "    {\"output\": f\"{review}\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human asked how they could help the AI, and the AI requested an analysis of a review. The human agreed to provide the review.\n",
      "AI: I ordered Pizza Salami for 9.99$ and it was awesome! The pizza was delivered on time and was still hot when I received it. The crust was thin and crispy, and the toppings were fresh and flavorful. The Salami was well-cooked and complemented the cheese perfectly. The price was reasonable and I believe I got my money's worth. Overall, I am very satisfied with my order and I would recommend this pizza place to others.\n",
      "Human: Thank you very much!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" You're welcome! It's always a pleasure to share my experiences. Is there anything else I can help you with?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Thank you very much!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: \\n\\nThe human asked how they could help the AI, and the AI requested an analysis of a review. The human agreed to provide the review, which was a positive review of a pizza order with Salami for 9.99$. The pizza was delivered on time and was still hot, with a thin and crispy crust and fresh and flavorful toppings. The Salami was well-cooked and complemented the cheese perfectly. The price was reasonable and the customer was very satisfied with their order, recommending the pizza place to others.\\nHuman: Thank you very much!\\nAI:  You're welcome! It's always a pleasure to share my experiences. Is there anything else I can help you with?\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
